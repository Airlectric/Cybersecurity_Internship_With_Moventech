{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7670451,"sourceType":"datasetVersion","datasetId":4473813}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Request-Oriented Sampling Method for DoW Attack Detection\n\nThe Request-Oriented Sampling Method is a technique used to detect denial-of-service (DoS) attacks by monitoring incoming requests to a system and analyzing various metrics associated with them. This method focuses on identifying anomalies in request patterns that may indicate a DoW attack.\n\n## Model Description\n\nThe model used for DoW attack detection is based on a combination of function-metric coordination features and hardware metrics. It employs a neural network architecture consisting of fully connected layers to predict whether incoming requests are part of a DoS attack or not. The model is trained using historical data on request metrics and hardware characteristics, and it aims to distinguish between normal traffic patterns and suspicious activity indicative of a DoW attack.\n\nThe architecture of the model includes input layers for function-metric coordination features and hardware metrics, followed by fully connected layers for feature extraction and a final output layer for classification. The model is trained using binary cross-entropy loss and optimized using the Adam optimizer.\n\nOnce trained, the model is evaluated using test data and various performance metrics such as accuracy, precision, and receiver operating characteristic (ROC) curve analysis. Additionally, the model's inference time and performance overhead are measured to assess its efficiency and effectiveness in real-world deployment.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Concatenate\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Dataset\n- The dataset contains network traffic data, which is crucial for detecting potential Denial of Wallet (DoW) attacks.","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/network-traffic-for-dos-detection/dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T21:43:43.813766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode Categorical Variables\n- Categorical variables like 'vmcategory' are encoded into numerical labels using `LabelEncoder`. This conversion is necessary as machine learning models require numerical inputs.\n","metadata":{}},{"cell_type":"code","source":"# Convert categorical variables to numerical labels\nlabel_encoder = LabelEncoder()\ndf['vmcategory'] = label_encoder.fit_transform(df['vmcategory'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define relevant features\nselected_features = ['RTT', 'InvocationDelay', 'ResponseDelay', 'FunctionDuration', 'ActiveFunctionsAtRequest', \n                     'ActiveFunctionsAtResponse', 'maxcpu', 'avgcpu', 'p95maxcpu', 'vmcorecountbucket', 'vmmemorybucket']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize Features\n- Features such as 'RTT', 'InvocationDelay', etc., are normalized using `MinMaxScaler`. Normalization ensures that all features have a similar scale, preventing some features from dominating others during model training.","metadata":{}},{"cell_type":"code","source":"# Normalize selected features\nscaler = MinMaxScaler()\ndf[selected_features] = scaler.fit_transform(df[selected_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Function-Metric Coordination Features\n- High-level function features (e.g., request/response delays) and low-level hardware metric features (e.g., CPU usage) are identified. These features collectively capture the behavior of network functions and system resources.","metadata":{}},{"cell_type":"code","source":"# Define function-metric coordination features\nfunction_metric_features = ['RTT', 'InvocationDelay', 'ResponseDelay', 'FunctionDuration', \n                            'ActiveFunctionsAtRequest', 'ActiveFunctionsAtResponse']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concatenate Function and Metric Features\n- Function features and hardware metric features are concatenated into a single input array. This integration allows the model to learn the relationship between application-level behavior and underlying system metrics, enhancing its predictive capability.","metadata":{}},{"cell_type":"code","source":"# Concatenate function and metric features\nX_function_metric = df[function_metric_features].values\nX_hardware_metrics = df[['maxcpu', 'avgcpu', 'p95maxcpu', 'vmcorecountbucket', 'vmmemorybucket']].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define input layers for function and hardware metrics\ninput_function_metric = Input(shape=(X_function_metric.shape[1],))\ninput_hardware_metrics = Input(shape=(X_hardware_metrics.shape[1],))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate inputs\nmerged_inputs = Concatenate()([input_function_metric, input_hardware_metrics])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Model Architecture\n- A neural network model is chosen for its ability to capture complex patterns in the data.\n- The model architecture consists of input layers for function and hardware metrics, followed by fully connected layers with ReLU activation.\n- ReLU activation helps introduce non-linearity, enabling the model to learn complex relationships between features.\n- The output layer with sigmoid activation predicts the probability of a DoS attack, as it outputs values between 0 and 1.","metadata":{}},{"cell_type":"code","source":"# Define fully connected layers\ndense_layer_1 = Dense(64, activation='relu')(merged_inputs)\ndense_layer_2 = Dense(32, activation='relu')(dense_layer_1)\n\n# Output layer\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\n# Define model\nmodel = Model(inputs=[input_function_metric, input_hardware_metrics], outputs=output_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compile Model\n- The model is compiled with binary crossentropy loss and Adam optimizer. Binary crossentropy is suitable for binary classification problems like DoW detection, while Adam optimizer efficiently updates model weights during training.","metadata":{}},{"cell_type":"code","source":"# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare Data for Training\n- The dataset is split into training and testing sets using `train_test_split`. This ensures that the model's performance is evaluated on unseen data, providing a more reliable assessment of its generalization capability.","metadata":{}},{"cell_type":"code","source":"# Prepare data\nX_function_metric_train, X_function_metric_test, X_hardware_metrics_train, X_hardware_metrics_test, y_train, y_test = train_test_split(\n    X_function_metric, X_hardware_metrics, df['bot'].values, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model\n- The model is trained on the training data for 10 epochs with a batch size of 32. Training involves adjusting model weights to minimize the difference between predicted and actual labels, thereby improving model accuracy.","metadata":{}},{"cell_type":"code","source":"# Train model\nhistory = model.fit([X_function_metric_train, X_hardware_metrics_train], y_train, epochs=10, batch_size=32, validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Model\n- The trained model is evaluated on the testing data to assess its performance in a real-world scenario. Evaluation metrics such as test loss and accuracy provide insights into the model's effectiveness.","metadata":{}},{"cell_type":"code","source":"# Evaluate model\ntest_loss, test_accuracy = model.evaluate([X_function_metric_test, X_hardware_metrics_test], y_test)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history\nplt.plot(history.history['accuracy'], label='Train Accuracy')\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Training History')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Calculate the percentage of DoW attacks based on the testing set\ntotal_instances_test = len(y_test)\ndos_percentage_test = np.sum(y_test) / total_instances_test * 100\nnormal_percentage_test = 100 - dos_percentage_test\n\n# Pie chart for the distribution of DoW attacks in the testing set\nlabels = ['Normal Instances', 'Potential DoW Attacks']\nsizes = [normal_percentage_test, dos_percentage_test]\ncolors = ['blue', 'red']  # Using a contrasting color for DoS attacks\nexplode = (0, 0.1)  # Explode the 2nd slice (i.e., 'Potential DoS Attacks')\n\nplt.figure(figsize=(8, 6))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of DoW Attacks in Testing Set')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict Probabilities\n- Probabilities of DoS attacks are predicted for the test set. Predicted probabilities allow for a more nuanced understanding of model confidence in its predictions.\n\n### Predict Binary Labels\n- Binary labels (0 or 1) are predicted based on a probability threshold of 0.5. This threshold determines whether a sample is classified as a DoS attack or not.","metadata":{}},{"cell_type":"code","source":"# Predict probabilities for test set\ny_pred_probs = model.predict([X_function_metric_test, X_hardware_metrics_test])\n\n# Predict binary labels based on probability threshold\ny_pred = (y_pred_probs > 0.5).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the cumulative distribution function (CDF)\nplt.figure(figsize=(10, 6))\nplt.hist(normal_probs, bins=100, color='blue', alpha=0.7, label='Normal', cumulative=True, density=True, histtype='step')\nplt.hist(dos_probs, bins=100, color='red', alpha=0.7, label='DoS', cumulative=True, density=True, histtype='step')\nplt.title('Cumulative Distribution of Predicted Probabilities')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Confusion Matrix\n- Confusion matrix is calculated to evaluate model performance. It provides a detailed breakdown of true positive, true negative, false positive, and false negative predictions, offering insights into model errors.\n\n### Calculate Accuracy and Precision\n- Accuracy measures the proportion of correctly classified samples, while precision measures the proportion of true positive predictions among all positive predictions. These metrics quantify the model's ability to make correct predictions and minimize false positives.\n\n### Calculate ROC Curve\n- Receiver Operating Characteristic (ROC) curve and Area Under Curve (AUC) are calculated to evaluate model performance across different probability thresholds. ROC curve illustrates the trade-off between true positive rate and false positive rate, while AUC summarizes overall model performance.\n\n### Plot ROC Curve\n- ROC curve is plotted to visualize the model's performance. A higher AUC indicates better model discrimination ability, with values closer to 1 indicating superior performance.\n\n### Plot Confusion Matrix\n- Confusion matrix is plotted to visually inspect model predictions against actual labels. This visualization aids in understanding model errors and identifying areas for improvement.\n\n### Plot Detection of False Positives and True Positives\n- Predicted probabilities and binary labels are visualized to identify false positives and true positives. This analysis helps stakeholders understand the model's behavior in distinguishing between normal network traffic and potential DoS attacks.","metadata":{}},{"cell_type":"code","source":"# # Calculate confusion matrix\n# conf_matrix = confusion_matrix(y_test, y_pred)\n# print(\"Confusion Matrix:\")\n# print(conf_matrix)\n\n# Calculate accuracy and precision\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\nroc_auc = auc(fpr, tpr)\nprint(\"ROC AUC:\", roc_auc)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\n#plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True)\n#plt.colorbar()\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.xticks([0.5, 1.5], ['Normal', 'Bot'])\nplt.yticks([0.5, 1.5], ['Normal', 'Bot'])\nplt.show()\n\n\n# Define colors based on true labels\ncolors = np.where(y_pred.flatten() == 1, 'blue', 'red')\n\n# Plot bar graph\nplt.figure(figsize=(12, 6))\nplt.bar(sample_indices, y_pred_probs.flatten(), color=colors)\nplt.title('Detection of False Positives and True Positives')\nplt.xlabel('Sample Index')\nplt.ylabel('Predicted Probability')\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n# Function to calculate inference time\ndef calculate_inference_time(model, X):\n    start_time = time.time()\n    model.predict(X)  # Make predictions\n    end_time = time.time()\n    inference_time = end_time - start_time\n    return inference_time\n\n# Function to calculate model prediction delay\ndef calculate_prediction_delay(model, X):\n    start_time = time.time()\n    model.predict(X)  # Make predictions\n    end_time = time.time()\n    prediction_delay = end_time - start_time\n    return prediction_delay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of X_function_metric_test and X_hardware_metrics_test\nprint(\"X_function_metric_test shape:\", X_function_metric_test.shape)\nprint(\"X_hardware_metrics_test shape:\", X_hardware_metrics_test.shape)\n\n# Check the data in X_function_metric_test and X_hardware_metrics_test\nprint(\"X_function_metric_test data:\", X_function_metric_test)\nprint(\"X_hardware_metrics_test data:\", X_hardware_metrics_test)\n\n# Define function to calculate inference time\ndef calculate_inference_time(model, X):\n    start_time = time.time()\n    model.predict(X)  # Make predictions\n    end_time = time.time()\n    inference_time = end_time - start_time\n    return inference_time\n\n# Calculate inference time with model running\ninference_time_with_model = calculate_inference_time(model, [X_function_metric_test, X_hardware_metrics_test])\n\n# Create an empty input for simulating no processing\nempty_input_function_metric = np.empty((1,) + X_function_metric_test.shape[1:])\nempty_input_hardware_metrics = np.empty((1,) + X_hardware_metrics_test.shape[1:])\n\n# Calculate inference time without model running \ninference_time_without_model = calculate_inference_time(model, [empty_input_function_metric, empty_input_hardware_metrics])\n\n# Calculate performance overhead\nperformance_overhead = (inference_time_with_model - inference_time_without_model) / inference_time_without_model * 100\n\nprint(\"Inference time with model running: {:.4f} seconds\".format(inference_time_with_model))\nprint(\"Inference time without model running: {:.4f} seconds\".format(inference_time_without_model))\nprint(\"Performance Overhead (%): {:.2f}%\".format(performance_overhead))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate prediction delay with model running\nprediction_delay_with_model = calculate_prediction_delay(model, [X_function_metric_test, X_hardware_metrics_test])\n\nprint(\"Prediction Delay with Model Running: {:.4f} seconds\".format(prediction_delay_with_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}